{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "Ans1:- Unsupervised learning in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction.\n",
        "\n",
        "Ans2:- The k-means clustering algorithm operates by categorizing data points into clusters by using a mathematical distance measure, usually euclidean, from the cluster center. The objective is to minimize the sum of distances between data points and their assigned clusters\n",
        "\n",
        "Ans3:- A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation. The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending on the selected distance measure.\n",
        "\n",
        "Ans4:- A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical\n",
        "\n",
        "Ans5:- K-Means tends to work well when the data is well-separated and evenly distributed, while DBSCAN is better suited for datasets with irregular shapes or varying densities. In the next sections, we'll dive deeper into each algorithm and learn how to implement them in Python using scikit-learn\n",
        "\n",
        "Ans6:- The silhouette score is specialized for measuring cluster quality when the clusters are convex-shaped, and may not perform well if the data clusters have irregular shapes or are of varying sizes. The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.\n",
        "\n",
        "Ans7:- One of the key drawbacks is its computational inefficiency, especially with large datasets, as the algorithm requires calculating distances between all pairs of data points, resulting in high time and memory complexity.\n",
        "\n",
        "Ans8:-The results of an experimental study show that, for features with different units, scaling them before k-means clustering provided better accuracy, precision, recall, and F-score values than when using the raw data.\n",
        "\n",
        "Ans9:- The algorithm works by defining clusters as dense regions separated by regions of lower density. This approach allows DBSCAN to discover clusters of arbitrary shape and identify outliers as noise.\n",
        "\n",
        "Ans10:- K-Means: Inertia\n",
        "It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia AND a low number of clusters ( K ). However, this is a tradeoff because as K increases, inertia decreases.\n",
        "\n",
        "Ans11:- The elbow method is a graphical method for finding the optimal K value in a k-means clustering algorithm. The elbow graph shows the within-cluster-sum-of-square (WCSS) values on the y-axis corresponding to the different values of K (on the x-axis). The optimal K value is the point at which the graph forms an elbow.\n",
        "\n",
        "Ans12:- DBSCAN works by partitioning the data into dense regions of points that are separated by less dense areas. It defines clusters as areas of the data set where there are many points close to each other, while the points that are far from any cluster are considered outliers or noise.\n",
        "\n",
        "Ans13:- It can be applied to both numerical and categorical data, but the latter requires some special considerations. In this blog post, we will explore how to perform hierarchical clustering on categorical data in Python using different methods and metrics.\n",
        "\n",
        "Ans14:- Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
        "\n",
        "Ans15:- It determines the distance between sets of observations as a function of the pairwise distance between observations. Furthermore, in Single Linkage Clustering, the distance between two clusters is the minimum distance between members of the two clusters\n",
        "\n",
        "Ans16:- K-means fails to find a meaningful solution, because, unlike MAP-DP, it cannot adapt to different cluster densities, even when the clusters are spherical, have equal radii and are well-separated.\n",
        "\n",
        "Ans17:- The two main parameters in DBSCAN are epsilon (Îµ), which defines the maximum distance between two points to be considered neighbors, and MinPts, which specifies the minimum number of points required to form a dense region.\n",
        "\n",
        "Ans18:- Like K-means, it is an unsupervised learning algorithm used to group similar data points together based on their similarity. The goal of K-means++ is to initialize the cluster centers in a more intelligent way than the random initialization used by K-means, which can lead to suboptimal results.\n",
        "\n",
        "Ans19:- Agglomerative clustering is defined as a hierarchical clustering method where items are grouped into clusters based on similarities, starting with each item as a singleton cluster and then merging pairs of clusters until all items are in one large cluster\n",
        "\n",
        "Ans20:- A high silhouette score indicates well-clustered data. Unlike inertia, the silhouette score provides more nuanced insight into the separation distance between the resulting clusters\n"
      ],
      "metadata": {
        "id": "biKJDl2xsNU1"
      }
    }
  ]
}